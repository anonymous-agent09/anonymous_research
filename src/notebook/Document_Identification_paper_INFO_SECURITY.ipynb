{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuVgE_UMcXlo"
      },
      "outputs": [],
      "source": [
        "print('Hello Google')\n",
        "!pip install evaluate\n",
        "!pip install accelerate -U\n",
        "!pip install transformers[torch]\n",
        "!pip install torchinfo\n",
        "import torch\n",
        "\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch._dynamo\n",
        "\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from torchinfo import summary\n",
        "\n",
        "import evaluate\n",
        "\n",
        "metric_acc = evaluate.load(\"accuracy\")  #\n",
        "metric_f1 = evaluate.load(\"f1\")\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = None\n",
        "\n",
        "# Tokenize helper function\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch['text'], padding=True, truncation=True, max_length=128)\n",
        "\n",
        "\n",
        "def get_tokenizer(model_path):\n",
        "    bert_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "    return bert_tokenizer\n",
        "\n",
        "\n",
        "def compute_metrics1(eval_pred, metric=metric_acc):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    \"\"\"\n",
        "    Computes accuracy, F1, precision, and recall for a given set of predictions.\n",
        "\n",
        "    Args:\n",
        "        pred (obj): An object containing label_ids and predictions attributes.\n",
        "            - label_ids (array-like): A 1D array of true class labels.\n",
        "            - predictions (array-like): A 2D array where each row represents\n",
        "              an observation, and each column represents the probability of\n",
        "              that observation belonging to a certain class.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the following metrics:\n",
        "            - Accuracy (float): The proportion of correctly classified instances.\n",
        "            - F1 (float): The macro F1 score, which is the harmonic mean of precision\n",
        "              and recall. Macro averaging calculates the metric independently for\n",
        "              each class and then takes the average.\n",
        "            - Precision (float): The macro precision, which is the number of true\n",
        "              positives divided by the sum of true positives and false positives.\n",
        "            - Recall (float): The macro recall, which is the number of true positives\n",
        "              divided by the sum of true positives and false negatives.\n",
        "    \"\"\"\n",
        "    # Extract true labels from the input object\n",
        "    labels = pred.label_ids\n",
        "\n",
        "    # Obtain predicted class labels by finding the column index with the maximum probability\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "\n",
        "    # Compute macro precision, recall, and F1 score using sklearn's precision_recall_fscore_support function\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
        "\n",
        "    # Calculate the accuracy score using sklearn's accuracy_score function\n",
        "    acc = accuracy_score(labels, preds)\n",
        "\n",
        "    # Return the computed metrics as a dictionary\n",
        "    return {\n",
        "        'Accuracy': acc,\n",
        "        'F1': f1,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall\n",
        "    }\n",
        "\n",
        "\n",
        "def train_classifier(model_path: str,\n",
        "                     dataset,\n",
        "                     output_dir=\"output\",\n",
        "                     train_batch_size=16,\n",
        "                     eval_batch_size=8,\n",
        "                     learning_rate= 5e-7, #1.25e-5\n",
        "                     num_epochs=10,\n",
        "                     metric_for_best_model=\"accuracy\"\n",
        "                     ):\n",
        "    dataset = dataset.rename_column(\"label\", \"labels\")  # to match Trainer\n",
        "    print(dataset)\n",
        "    tokenized_dataset = dataset.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
        "    print(tokenized_dataset[\"train\"].features.keys())\n",
        "\n",
        "    # Prepare model labels - useful for inference\n",
        "    num_labels = 2\n",
        "    id2label = {0: \"NON_INFOSEC\", 1: \"INFOSEC\"}\n",
        "    label2id = {\"NON_INFOSEC\": 0, \"INFOSEC\": 1}\n",
        "\n",
        "    # Fine-tune & evaluate\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_path,\n",
        "        num_labels=num_labels,\n",
        "        id2label=id2label,\n",
        "        label2id=label2id,\n",
        "        hidden_dropout_prob=0.3,\n",
        "        attention_probs_dropout_prob=0.25\n",
        "    )\n",
        "\n",
        "    for param in model.parameters(): param.data = param.data.contiguous()\n",
        "\n",
        "    print(\" ############ Model Summary ######\")\n",
        "    print(model.cuda())\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        per_device_train_batch_size=train_batch_size,\n",
        "        per_device_eval_batch_size=eval_batch_size,\n",
        "        learning_rate=learning_rate,\n",
        "        lr_scheduler_type='linear', #constant constant_with_warmup\n",
        "        warmup_steps=0,\n",
        "        num_train_epochs=num_epochs,\n",
        "        torch_compile=True,  # optimizations\n",
        "        optim=\"adamw_torch\",  # improved optimizer\n",
        "        # logging & evaluation strategies\n",
        "        # logging_dir=f\"{repository_id}/logs\",\n",
        "        logging_strategy=\"steps\",\n",
        "        logging_steps=100,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        weight_decay=0.00, # prevent overfitting default 0.01\n",
        "        #fp16=True,\n",
        "        save_total_limit=2,\n",
        "        load_best_model_at_end=True,\n",
        "        #metric_for_best_model=metric_for_best_model,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset[\"train\"],\n",
        "        eval_dataset=tokenized_dataset[\"test\"],\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    return model, trainer\n",
        "\n"
      ],
      "metadata": {
        "id": "Wf5NUXaSci9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "def prepare_trainingset(dataset_file_path:str, test_size=0.25):\n",
        "    dataset = load_dataset(\"csv\", data_files=dataset_file_path)\n",
        "    dataset = dataset['train'].train_test_split(test_size=test_size, shuffle=True)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "1ctqckRTcjJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"allenai/scibert_scivocab_uncased\" #\"anferico/bert-for-patents\"\n",
        "tokenizer = get_tokenizer(model_path)\n",
        "\n",
        "\n",
        "dataset = prepare_trainingset('info_sec_training_dataset_0_1.csv')\n"
      ],
      "metadata": {
        "id": "gbLdOVCKcnR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, trainer = train_classifier(model_path, dataset, num_epochs=10)"
      ],
      "metadata": {
        "id": "1bQ4AQNmWLTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.num_parameters())"
      ],
      "metadata": {
        "id": "1QWkBm3UJueK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save the model\n",
        "def save_model(model_dir_path:str, trainer, tokenizer):\n",
        "  trainer.save_model(model_dir_path)\n",
        "  tokenizer.save_pretrained(model_dir_path)\n",
        "  print('Model is saved ..')\n",
        "\n",
        "save_model(\"infosec_model\", trainer, tokenizer)"
      ],
      "metadata": {
        "id": "pqvcaMNCcpFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification, TextClassificationPipeline, pipeline\n",
        "import pandas as pd\n",
        "\n",
        "import evaluate\n",
        "from evaluate import evaluator\n",
        "from datasets import Dataset\n",
        "\n",
        "def evaluate_model(test_data_path, model_path):\n",
        "    pipe = pipeline(\n",
        "        \"text-classification\", model=model_path, max_length=128\n",
        "    )\n",
        "\n",
        "    # Define dataset\n",
        "    test_data = pd.read_csv(test_data_path)\n",
        "    test_dataset = Dataset.from_pandas(test_data)\n",
        "\n",
        "    # Define evaluator\n",
        "    accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "    # Evaluate accuracy\n",
        "    eval = evaluator(\"text-classification\")\n",
        "    acc_result = eval.compute(\n",
        "        model_or_pipeline=pipe,\n",
        "        data=test_dataset,\n",
        "        metric=accuracy,\n",
        "        label_mapping={\"NON_INFOSEC\": 0, \"INFOSEC\": 1},\n",
        "        strategy=\"bootstrap\",\n",
        "        n_resamples=100,\n",
        "    )\n",
        "\n",
        "    # Evaluate F1 score\n",
        "    f1_metric = evaluate.load(\"f1\")\n",
        "    f1_result = eval.compute(\n",
        "        model_or_pipeline=pipe,\n",
        "        data=test_dataset,\n",
        "        metric=f1_metric,\n",
        "        label_mapping={\"NON_INFOSEC\": 0, \"INFOSEC\": 1},\n",
        "        strategy=\"bootstrap\",\n",
        "        n_resamples=100,\n",
        "    )\n",
        "\n",
        "    return acc_result, f1_result"
      ],
      "metadata": {
        "id": "uBmECEvHctjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc, f = evaluate_model(\"plasma_test_data_annotated_.csv\", \"plasma_model\")"
      ],
      "metadata": {
        "id": "AdyOEyYVcvoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc"
      ],
      "metadata": {
        "id": "Kt1Jiz3gcxv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f"
      ],
      "metadata": {
        "id": "MXAilwZMczlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"plasma_model/\"\n",
        "\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "tokenizer= AutoTokenizer.from_pretrained(model_path)\n",
        "fined_model= pipeline(\"text-classification\", model=model, tokenizer=tokenizer, truncation=True, max_length=128)"
      ],
      "metadata": {
        "id": "-uesRw4FdgfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['test']"
      ],
      "metadata": {
        "id": "Ft-qn87-udzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dDLG1OaQoqpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QjXgOp8ZfJtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = fined_model(dataset['test']['text'])\n",
        "predictions[:5]\n"
      ],
      "metadata": {
        "id": "o0YXVwO1ud_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seaborn"
      ],
      "metadata": {
        "id": "Fv_ZUzuww-b7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "def get_label(d):\n",
        "  if d['label'] == 'PLASMA':\n",
        "    return 0\n",
        "  else:\n",
        "    return 1\n",
        "\n",
        "\n",
        "\n",
        "predictions = [get_label(d) for d in predictions]\n",
        "\n",
        "\n",
        "print(\"acc:\",accuracy_score(dataset['test']['label'], predictions))\n",
        "print(\"f1:\",f1_score(dataset['test']['label'], predictions, average = 'macro'))\n",
        "\n",
        "# create function for plotting confusion matrix\n",
        "def plot_cm(cm):\n",
        "  classes = ['PLASMA','NO_PLASMA']\n",
        "  df_cm = pd.DataFrame(cm, index=classes, columns=classes)\n",
        "  ax = sns.heatmap(df_cm, annot = True, fmt='g')\n",
        "  ax.set_xlabel('Predicted')\n",
        "  ax.set_ylabel('Actual')\n",
        "\n",
        "cm = confusion_matrix(dataset['test']['label'],predictions, normalize = 'true')\n",
        "plot_cm(cm)\n"
      ],
      "metadata": {
        "id": "lT3bq4A2wKF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fined_model(\"The present invention relates to ionized gas with blood plasma.\")"
      ],
      "metadata": {
        "id": "Lvr7gFssZU8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fined_model(\"The present invention relates to co2 gas.\")"
      ],
      "metadata": {
        "id": "l8_ReLlJa3zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fined_model(\"The present invention relates tocancer tratment by using plasma.\")"
      ],
      "metadata": {
        "id": "Qp9HlpmHYcaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fined_model(\"The present invention relates to an electromagnetic pulse protection method and an electromagnetic pulse protection system\")"
      ],
      "metadata": {
        "id": "KicAzyFQZC9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fined_model(\"The present invention relates to surface claening by microwave plasma\")"
      ],
      "metadata": {
        "id": "kCBiXql-bIlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fined_model(\"The present disclosure generally relates to a surface cleaning apparatus\")"
      ],
      "metadata": {
        "id": "f_sIpOasbUJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "metadata": {
        "id": "nTWsE2_ibpbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"plasma_test_data_annotated_.csv\", encoding=\"utf-8\")\n",
        "\n",
        "predictions_df = fined_model(df['text'].tolist())\n",
        "#predictions_df[:5]\n"
      ],
      "metadata": {
        "id": "Zi-GbHYerhFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_df[:50]"
      ],
      "metadata": {
        "id": "6H5CFa3fszHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "def get_label(d):\n",
        "  if d['label'] == 'PLASMA':\n",
        "    return 0\n",
        "  else:\n",
        "    return 1\n",
        "\n",
        "\n",
        "\n",
        "predictions = [get_label(d) for d in predictions_df]\n",
        "\n",
        "\n",
        "print(\"acc:\",accuracy_score(df['label'].tolist(), predictions))\n",
        "print(\"f1:\",f1_score(df['label'].tolist(), predictions, average = 'macro'))\n",
        "\n",
        "# create function for plotting confusion matrix\n",
        "def plot_cm(cm):\n",
        "  classes = ['PLASMA','NO_PLASMA']\n",
        "  df_cm = pd.DataFrame(cm, index=classes, columns=classes)\n",
        "  ax = sns.heatmap(df_cm, annot = True, fmt='g')\n",
        "  ax.set_xlabel('Predicted')\n",
        "  ax.set_ylabel('Actual')\n",
        "\n",
        "cm = confusion_matrix(df['label'].tolist(),predictions, normalize = 'true')\n",
        "plot_cm(cm)\n"
      ],
      "metadata": {
        "id": "UTO8wEuQrzdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from sklearn.metrics import roc_curve,confusion_matrix,auc\n"
      ],
      "metadata": {
        "id": "ulR-IQFbs2Df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "mpl.rcParams['figure.figsize'] = (12, 10)\n",
        "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
        "\n",
        "def plot_loss(history):\n",
        "# Use a log scale to show the wide range of values.\n",
        "    plt.semilogy(history.epoch,  history.history['loss'],\n",
        "               color='red', label='Train Loss')\n",
        "    plt.semilogy(history.epoch,  history.history['val_loss'],\n",
        "          color='green', label='Val Loss',\n",
        "          linestyle=\"--\")\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "def plot_metrics(history):\n",
        "    metrics =  ['loss', 'auc', 'precision', 'recall']\n",
        "    for n, metric in enumerate(metrics):\n",
        "        name = metric.replace(\"_\",\" \").capitalize()\n",
        "        plt.subplot(2,2,n+1)\n",
        "        plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n",
        "        plt.plot(history.epoch, history.history['val_'+metric],\n",
        "                 color=colors[0], linestyle=\"--\", label='Val')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel(name)\n",
        "        if metric == 'loss':\n",
        "            plt.ylim([0, plt.ylim()[1]])\n",
        "        elif metric == 'auc':\n",
        "            plt.ylim([0.8,1])\n",
        "        else:\n",
        "            plt.ylim([0,1])\n",
        "\n",
        "        plt.legend()\n",
        "\n",
        "def plot_cm(y_true, y_pred, title):\n",
        "    ''''\n",
        "    input y_true-Ground Truth Labels\n",
        "          y_pred-Predicted Value of Model\n",
        "          title-What Title to give to the confusion matrix\n",
        "\n",
        "    Draws a Confusion Matrix for better understanding of how the model is working\n",
        "\n",
        "    return None\n",
        "\n",
        "    '''\n",
        "\n",
        "    figsize=(10,10)\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n",
        "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
        "    cm_perc = cm / cm_sum.astype(float) * 100\n",
        "    annot = np.empty_like(cm).astype(str)\n",
        "    nrows, ncols = cm.shape\n",
        "    for i in range(nrows):\n",
        "        for j in range(ncols):\n",
        "            c = cm[i, j]\n",
        "            p = cm_perc[i, j]\n",
        "            if i == j:\n",
        "                s = cm_sum[i]\n",
        "                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
        "            elif c == 0:\n",
        "                annot[i, j] = ''\n",
        "            else:\n",
        "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
        "    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n",
        "    cm.index.name = 'Actual'\n",
        "    cm.columns.name = 'Predicted'\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    plt.title(title)\n",
        "    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)\n",
        "\n",
        "def roc_curve_plot(fpr,tpr,roc_auc):\n",
        "    plt.figure()\n",
        "    lw = 2\n",
        "    plt.plot(fpr, tpr, color='darkorange',\n",
        "             lw=lw, label='ROC curve (area = %0.2f)' %roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic example')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9TIgRZgDOlUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a7PBH5OfUcmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict_prob = []\n",
        "for element in predictions_df:\n",
        "  y_predict_prob.append(element['score'])\n"
      ],
      "metadata": {
        "id": "lJbLn742Uc6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_valid = df['label'].tolist()\n",
        "y_predict_prob = predictions\n",
        "fpr, tpr, _ = roc_curve(y_valid,y_predict_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "roc_curve_plot(fpr,tpr,roc_auc)"
      ],
      "metadata": {
        "id": "DzQxPVr7UnZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6iUye9GPYuN6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}